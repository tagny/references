# Revue de litt√©rature
Notes de lectures d'articles et de rapports scientifiques


<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [Revue de litt√©rature](#revue-de-littrature)
	- [Evaluation](#evaluation)
		- [2020-DamirKrstiniƒá-MultiLabelClassifPerfEvalWithConfusionMatrix.pdf](#2020-damirkrstini-multilabelclassifperfevalwithconfusionmatrixpdf)
	- [dataless intent recognition](#dataless-intent-recognition)
		- [2020-ExploitingClozeQuestionsForFewShotTextClassifAndNLI-Schick](#2020-ExploitingClozeQuestionsForFewShotTextClassifAndNLI-Schick)
		- [2019-WenpengYin-BenchmarkingZeroshotTextClassificationDatasetsEvaluationAndEntailmentApproach](#2019-wenpengyin-benchmarkingzeroshottextclassificationdatasetsevaluationandentailmentapproach)
		- [2020-YuMeng-TextClassifUsingLabelNamesOnlyALangModelSelfTrainingApproach](#2020-yumeng-textclassifusinglabelnamesonlyalangmodelselftrainingapproach)
		- [2018 Doc2Cube - Allocating Documents to Text Cube without Labeled Data [En cours]](#2018-doc2cube-allocating-documents-to-text-cube-without-labeled-data-en-cours)
		- [2018 DatalessTextClassificationATopicModelingApproachwith DocumentManifold [En cours]](#2018-datalesstextclassificationatopicmodelingapproachwith-documentmanifold-en-cours)
		- [2018 - [GOOD] web - How to Build Your Own Text Classification Model Without Any Training Data [FINI][MISE EN PRATIQUE INTERROMPUE]](#2018-good-web-how-to-build-your-own-text-classification-model-without-any-training-data-finimise-en-pratique-interrompue)
		- [2019 Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings [FINI][A RESUMER]](#2019-towards-unsupervised-text-classification-leveraging-experts-and-word-embeddings-finia-resumer)
		- [2020 Early Forecasting of Text Classification Accuracyand F-Measure with Active Learning [EN COURS]](#2020-early-forecasting-of-text-classification-accuracyand-f-measure-with-active-learning-en-cours)
		- [2015 slides - Text Classification without Supervision - Incorporating World Knowledge and Domain Adaptation [EN COURS]](#2015-slides-text-classification-without-supervision-incorporating-world-knowledge-and-domain-adaptation-en-cours)
		- [2014 Transfer Understanding from Head Queries to Tail Queries [EN COURS]](#2014-transfer-understanding-from-head-queries-to-tail-queries-en-cours)
		- [2019 - slide - NLP from scratch - Solving the cold start problem for NLP [FINI]](#2019-slide-nlp-from-scratch-solving-the-cold-start-problem-for-nlp-fini)
		- [2019_ESWC_KB_Short_Text_Classification_Using_Entity_and_Category_Embedding [FINI]](#2019eswckbshorttextclassificationusingentityandcategoryembedding-fini)
		- [2020 Description Based Text Classification with Reinforcement Learning [MIS DE COTE]](#2020-description-based-text-classification-with-reinforcement-learning-mis-de-cote)
		- [2018 A Pseudo Label based Dataless NB Bayes Algorithm for Text Classification with Seed Words [EN COURS]](#2018-a-pseudo-label-based-dataless-nb-bayes-algorithm-for-text-classification-with-seed-words-en-cours)
		- [medium_com_ai_medecindirect_unsupervised_text_classification [WEB][FINI]](#mediumcomaimedecindirectunsupervisedtextclassification-webfini)
	- [dataless annotation/data augmentation](#dataless-annotationdata-augmentation)
		- [2020 When does data augmentation help generalization in NLP [EN COURS][PRIORITAIRE]](#2020-when-does-data-augmentation-help-generalization-in-nlp-en-coursprioritaire)
		- [2019 EDA - Easy Data Augment Tech for Boosting Perf on Text Classif [En cours][PRIORITAIRE]8](#2019-eda-easy-data-augment-tech-for-boosting-perf-on-text-classif-en-coursprioritaire8)
	- [dataless annotation/Weak supervision](#dataless-annotationweak-supervision)
		- [2019 A clinical text classification paradigm using weak supervision and deep representation [FINI]](#2019-a-clinical-text-classification-paradigm-using-weak-supervision-and-deep-representation-fini)
	- [embeddings](#embeddings)
		- [2014 GloVe - Global Vectors for Word Representation](#2014-glove-global-vectors-for-word-representation)
		- [2017 SIF-a_simple_but_tough_to_beat_baseline_for_sentence_embeddings [EN COURS]](#2017-sif-asimplebuttoughtobeatbaselineforsentenceembeddings-en-cours)
		- [2018-sent2vec](#2018-sent2vec)
		- [2018-ImprovingLanguageUnderstandingByGenerativePreTraining-RadfordNarasimhanSalimansSutskever-GPT](#2018-improvinglanguageunderstandingbygenerativepretraining-radfordnarasimhansalimanssutskever-gpt)
		- [2014 doc2vec](#2014-doc2vec)
		- [2017 fasttext - Enriching Word Vectors with Subword Information](#2017-fasttext-enriching-word-vectors-with-subword-information)
		- [2020 P-SIF - Document Embeddings Using Partition Averaging [PRIORITAIRE]](#2020-p-sif-document-embeddings-using-partition-averaging-prioritaire)
		- [2015 [GOOD PERF] AdaSent-SelfAdaptiveHierarchicalSentenceModel [PRIORITAIRE]](#2015-good-perf-adasent-selfadaptivehierarchicalsentencemodel-prioritaire)
		- [2019 Bert - Pre-Training of Deep Bidirectional Transformers for Language understanding](#2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
		- [2020 CamemBERT - a Tasty French Language Model](#2020-camembert-a-tasty-french-language-model)
		- [2020 SBERT-WK - A Sentence Embedding Method By Dissecting BERT-based Word Models [PRIORITAIRE]](#2020-sbert-wk-a-sentence-embedding-method-by-dissecting-bert-based-word-models-prioritaire)
		- [2019 Sentence-BERT [PRIORITAIRE]](#2019-sentence-bert-prioritaire)
		- [2020 Improving Sentence Representations via Component Focusing (CF-BERT)](#2020-improving-sentence-representations-via-component-focusing-cf-bert)
		- [2019 spherical-text-embedding](#2019-spherical-text-embedding)
	- [knowledge graph](#knowledge-graph)
		- [2017 Graph-based_Text_Representations_Tutorial_EMNLP_2017 [TURORIEL][FINI][A RESUMER]](#2017-graph-basedtextrepresentationstutorialemnlp2017-turorielfinia-resumer)
	- [query analysis](#query-analysis)
		- [2018_Chapter_UnderstandingInformationNeeds [SUSPENDU]](#2018chapterunderstandinginformationneeds-suspendu)
			- [Analyse s√©mantique de requ√™te](#analyse-smantique-de-requte)
	- [short texts similarity](#short-texts-similarity)
		- [2019 Sentence Similarity Techniques for Short vs Variable Length Textusing Word Embeddings [FINI]*](#2019-sentence-similarity-techniques-for-short-vs-variable-length-textusing-word-embeddings-fini)
	- [short texts similarity / metric learning](#short-texts-similarity-metric-learning)
		- [2019 ATutorialonDistanceMetricLearning-MathematicalFoundationsAlgorithmsandExperiments [EN COURS]](#2019-atutorialondistancemetriclearning-mathematicalfoundationsalgorithmsandexperiments-en-cours)
		- [2019 Metric Learning for Dynamic Text Classification [EN COURS]](#2019-metric-learning-for-dynamic-text-classification-en-cours)
	- [misc-suggested](#misc-suggested)
		- [2020-LuyuGao-ModularizedTransfomerBasedRankingFramework](#2020-luyugao-modularizedtransfomerbasedrankingframework)
		- [2020-MujeenSung-BiomedicalEntityReprWithSynonymMarginalization](#2020-mujeensung-biomedicalentityreprwithsynonymmarginalization)
	- [speech recognition](#speech-recognition)
		- [2020-MLSALargeScaleMultilingualDatasetForSpeechResearch-VineelPratap](#2020-mlsalargescalemultilingualdatasetforspeechresearch-vineelpratap)
	- [TEMPLATE](#template)
		- [PDF FILE NAME](#pdf-file-name)

<!-- /TOC -->



## Evaluation

### 2020-DamirKrstiniƒá-MultiLabelClassifPerfEvalWithConfusionMatrix.pdf
**Probl√®me** : comment estimer et interpr√©ter l'efficacit√© d'une approche de classification multi-label (MLC) avec une matrice de confusion ?

**Solution**: l'article propose une nouvelle approche pour calculer la matrice de confusion pour la classif multi-label:
* l'approche comble les lacunes des approches existantes
* Gr√¢ce √† sa polyvalence (*abilit√© √† s'adapt√© ou √™tre adaptable √† diff√©rentes fonctions ou activit√©s*) elle peut donc √™tre employ√©e dans diff√©rents domaines

**contexte**
* Difficult√©s des algos MLC:
  * affecter plus d'un concept s√©mantique √† chaque instance
  * existence d'une correlation entre diff√©rents labels
  * nombre in√©gal d'occurrences de labels dans les donn√©es (d√©s√©quilibre d'annotation de train)
	* chaque label n'a pas le m√™me nombre d'instances : certains sont tr√®s sollicit√©s/courants quand d'autres peuvent √™tre rares en r√©alit√© et n'apparaitre que dans une tr√®s faible proportion des exemples
	* chaque instance n'a pas le m√™me nombre de labels
  * diff√©rents labels sont souvent tr√®s similaires dans le contexte de certaines instances ; ce qui rend difficile leur annotation non ambig√ºe m√™me par un humain
    * en plus de l'efficacit√© des algos, il faut **rev√©ler les relations entre labels** et **indiquer clairement la faiblesse du classifieur (les ktqs du pb qui biaise ou handicape l'algo)** ==> **gr√¢ce √† la matrice de confusion**
* La classif multi-label : entrainer une fonction pour pr√©dire un vecteur binaire √† dim defini par les labels (1 si le label est pertinent, 0 sinon
* Les **metriques couramment utilis√©es** distinguent l'√©val bas√©e sur les labels et celle bas√©e sur les exemples pour un syst√®me H et un jeu de test D de taille n et un nb de labels candidats q:
  * eval bas√©e sur les exemples :
    * Justesse_EB(H, D) = (1/n)(\sum_{i=1}^n |Yi \inter Z_i|/|Yi \union Z_i| : proportion des labels correctement pr√©dits
	* Precision_EB(H, D) = (1/n)(\sum_{i=1}^n |Yi \inter Z_i|/|Z_i| : proportion de labels correctement pr√©dits par le total de labels pr√©dits
	* Rappel_EB(H, D) = (1/n)(\sum_{i=1}^n |Yi \inter Z_i|/|Yi| : proportion de labels correctement pr√©dits par le total de labels attendus
	* F1_EB(H, D) = (2*Precision_EB*Rappel_EB)/(Precision_EB + Rappel_EB) =  (1/n)(\sum_{i=1}^n 2*|Yi \inter Z_i|/|Yi XOR Z_i|
	* HammingLoss_EB(H, D) = (1/n)(\sum_{i=1}^n |Yi \inter Z_i|/q : estime le nb moyen de fois o√π la pertinence d'un example pour un label est incorrectement pr√©dit i.e. il y a prise en compte de l'erreur de pr√©diction (un label incorrect est pr√©dit) et l'erreur de manquement (un label pertinent n'est pas pr√©dit).
	* Justesse de sous-ensemble (**subset accuracy**) ou correspondance exacte (**exact match**) : (1/n) \sum_{i=1}^n I(Y_i=Z_i)
  * eval bas√©e sur les labels : les labels sont consid√©r√©s comme s√©par√©s, r√©duisant le MLC √† un classifieur binaire pour chaque label avec des TP, TN, TN, FN
    * Justesse_j = (TP_j + TN_j) / (TP_j + TN_j + FP_j + FN_j)
	* Precision_j = (TP_j) / (TP_j + FP_j)
	* Rappel_j = (TP_j) / (TP_j + FN_j)
	* F1_j = (2*Precision_j*Rappel_j)/(Precision_j + Rappel_j) = 2*TP_j / (2TP_j + FP_j + FN_j)
	* Moyenne macro d'une metrique B: B_macro(H, D) = (1/q) \sum_{j=1}^q B_j
	Moyenne micro d'une metrique B: B_micro(H, D) = (1/q) B(\sum_{j=1}^q TP_j, \sum_{j=1}^q TN_j, \sum_{j=1}^q FP_j, \sum_{j=1}^q FN_j)
* La matrice de confusion est compl√©mentaire √† ces m√©triques car :
  * elle rev√®le que les instances d'un label sont souvent class√©s par erreur avec un autre label qu'on peut bien identifier ; ce qui **sugg√®re au concepteur d'am√©liorer l'algo en cherchant plus de ktqs qui pourraient aider √† mieux distinguer les labels confondus**.
  * son analyse pourrait :
    * fournir un aper√ßu des relations entre ktqs et objets de donn√©es diff√©rentes dfts
	* rev√©ler la structure inh√©rente des donn√©es.
* la matrice de confusion compare la classe pr√©dite d√©finissant une colonne et la classe attendue d√©finissant une ligne:
  * la cellule (Y, Z) est le nb de fois qu'un objet de la classe Y est affecter √† la classe Z
  * la diagonale repr√©sente le nb de classif pr√©cise pour chaque classe quand les √©lts hors-diagonal sont les erreurs de classif.
* On peut aussi **normaliser** la matrice de confusion:
  * la **matrice de rappel** est la division de chaque cellule par la somme de tous les √©lts de la m√™me ligne
    * en diagonal, on obtient le rappel de chaque classe
	* les autres √©l√©ments de la m√™me ligne, repr√©sente la probabilit√© qu'un objet de la classe Y (ligne) sera class√© par erreur dans Z (colonne)
  * la **matrice de pr√©cision** est la division de chaque cellule par la somme de tous les √©lts de la m√™me colonne
    * en diagonal, on obtient la pr√©cision de chaque classe
	* les autres √©l√©ments de la m√™me colonne, repr√©sente la probabilit√© qu'un objet class√© dans Z (colonne ou pr√©dit) est effectivement de la classe Y (ligne ou attendu)

**Algo propos√© pour la construction de la matrice de confusion en MLC**:
* **condition d'application** : chaque instance a au moins un label i.e. |Y|>0 **et** |Z|>0 ;  *peut-√™tre cr√©er un label OUT-OF-DOMAIN en plus pour les cas sans label*
* 4 sc√©narios pour une instance x :
  * ùëå = Z : C(x) = diag(Y)
  * |ùëå\ùëç| = 0, |ùëç\ùëå| > 0 : ùê∂ = [ùëå ‚äó (ùëç\ ùëå) + |ùëå| ‚ãÖ ùëëùëñùëéùëî(ùëå) ]/|ùëç| avec ‚äó = le outer product ??
  * |ùëå\ùëç| > 0, |ùëç\ùëå| = 0 : ùê∂ = [(ùëå\ùëç) ‚äó ùëç]/|ùëç| + ùëëùëñùëéùëî(ùëç)
  * |ùëå\ùëç| > 0, |ùëç \ùëå| > 0 : ùê∂ = [(ùëå\ùëç) ‚äó (ùëç\ùëå)]/|ùëç\ùëå| + ùëëùëñùëéùëî(ùëå ‚à© ùëç)
  
  

## dataless intent recognition
**Probl√®me** : absence de donn√©es annot√©es pour entrainer des algo de NLP

**Solutions**:
* **dataless text classification** : employer des connaissances (KB) √† la place des donn√©es
  * exploite la similarit√© (lexicale, syntaxique ou s√©mantique) entre le texte √† classifier et les cat√©gories (pareil qu'en RI)
  * bon si donn√©es non annot√©es indisponibles
* **weak supervision** : g√©n√©rer automatiquement des donn√©es (par exemple avec un syst√®me de r√®gles REGEX)
  * bon si textes brutes dispo et expertise de mots-cl√©s dispo


### 2020-ExploitingClozeQuestionsForFewShotTextClassifAndNLI-Schick

**Question pour orienter la lecture**: 
  * What is a cloze question?
  * What do cloze questions have to do with text classification and natural language inference?
  * The paper deals with few shot text classification and NLI, how does it do that?
  * Is the approach pretrainabled on an other task dataset for the purpose of a zero-shot learning ?
  * why do we need few training data ?
  * Is the source code provided ?
  * Are all the resources provided (pretrained model, data, ...) ? For what languages ?
  * Is it simple to apply it?
  * How good is the approach compared to others?
  
**Pbs**: 
* few-shot learning == leverage some few examples to address the limit of unsupervised learning solutions based on language models and task descriptions
* comment combiner la description d'une t√¢che et l'apprentissage supervis√© r√©gulier sur un faible dataset annot√© ?

**Approches pr√©c√©dentes**
* des solutions r√©centes sont propos√©es en zero-shot : voir (Radford
et al., 2019; Puri and Catanzaro, 2019)
* les approches pr√©c√©dentes utilisant des patrons le font pour d√©finir le mod√®le de langue et non pour am√©liorer une t√¢che sp√©cifique.

**proposition**: Entrainement par exploitation de patrons (motifs)
* reformulation de texte entr√©e en textes √† trous suivant des patrons du NLP

**voir aussi**
* https://towardsdatascience.com/gpt-3-vs-pet-not-big-but-beautiful-7a73d17af981
* http://timoschick.com/explanatory%20notes/2020/10/23/pattern-exploiting-training.html [DETAILED][MORE EXAMPLES]
* https://inderjit.in/post/pet_for_nlp/
* https://www.aclweb.org/anthology/2020.coling-main.488.pdf
* https://www.pragmatic.ml/pet/ [PEDAGOGIC]


### 2019-WenpengYin-BenchmarkingZeroshotTextClassificationDatasetsEvaluationAndEntailmentApproach
**Pb**: 0shot-tc == affecter le label appropri√© √† un morceau de texte ind√©pendament du domaine et de l'aspect textuel d√©crit par le label.
* **aucune donn√©e annot√©e pour l'entrainement pour une partie ou tout l'ens des labels**

**Pb de l'Existant**
* modelisation restrainte √† une seule t√¢che (cat√©gorisation th√©matique)
* les labels sont encod√©s en indices sans tenir compte de leur sens
* difficile de comparer les diff√©rent travaux
  * datasets diff√©rents
  * m√©triqs d'√©val diff√©rentes

**Proposition**: benchmark de datasets et d'√©val, et une approche par implication (*entailment*) pour g√©rer diff√©rents aspects (th√®me, √©motion, √©vts, ...) dans un paradigme unifi√©
1. d√©f plus g√©n√©ral/large du pb: oshot-tc == apprendre une fct f: X -> Y o√π f(.) n'a jamais vu de donn√©es annot√©es des labels de Y lors de son dev
2 Dataset :
  * 3 aspects (th√®me, √©motion, et situation == event).
  * donn√©es d√©coup√©es :
    * train, dev, test
    * et s√©paration des classes vues et classes non vues
3. Evaluation: ?

**voir aussi**
* code source : https://github.com/CogComp/BenchmarkingZeroShot
* d√©mo en ligne : https://cogcomp.seas.upenn.edu/page/demo_view/ZeroShotTextClassification
* Pr√©sentation √† EMNLP : https://vimeo.com/409401114

### 2020-YuMeng-TextClassifUsingLabelNamesOnlyALangModelSelfTrainingApproach
**Pb**:
* les algos actuels ont besoin d'un bon jeu de donn√©es annot√©es par des humains
  * difficile et cher d'en obtenir
* l'humain n'a pourtant pas besoin de donn√©es d'entrainement
  * mais souvent d'un petit ens de termes d√©crivant les cat√©gories

### 2018 Doc2Cube - Allocating Documents to Text Cube without Labeled Data [En cours]
** Objectifs ** : Construire un mod√®le Doc2Cube de cubes de textes √† partir d'un corpus textuel (D) automatiquement sans donn√©es annot√©es uniquement avec le label comme petit ensemble de termes semences
** Probl√®me **: difficile d'annoter suffisamment de documents pour la classification
** Applications ** :
* Faciliter les analyses textuelles multidimensionnelles
* r√©v√©ler la similarit√© s√©mantique entre les labels, les termes et les documents
* BI : facilit√© d'exploration du corpus et de recherche des passages/articles d√©sir√©s avec de simple requ√™tes
* biomedical: organisation de corpus en aspects maladies, g√®nes, prot√©ine;
* bioinfo : recherche plus facile d'article scientifique
** D√©finition **
* **cube de texte (text cube)** : structure multidimensionnelle de donn√©es contenant des documents textuels, o√π les dimensions correspondent √† plusieurs aspects (e.g. th√®me, temps, lieu) du corpus D.
* **sch√©ma pr√©d√©fini de cube C = (L_1, L_2, ..., L_n) = (th√®me, temps, lieu)** : ensemble des n dimensions du cube.
* celulle de cube (l_{t_1}, l_{t_2}, ..., l_{t_n}) : localisation des documents ayant les labels l_{t_i} dans les dimensions L_i resp.
* **T√¢che de construction de cube**: organiser un grand corpus de texte dans les classes d'un cube i.e. affecter n labels l_{t_1}, l_{t_2}, ..., l_{t_n} √† chaque document d o√π l_{t_i} \in L_i repr√©sente la cat√©gory de d dans la dimension L_i (L_i = Th√®me = {l_{t_1} = sport, l_{t_2} = business, ..., l_{t_n}=movies}

**Existant et limites**
* G√©n√©ration des donn√©es d'entrainement limit√© par l'exigence d'effort d'ing√©nierie des caract√©ristiques
* dataless classification limit√©e par le risque de limite et non adaptation du corpus externe
* application de la classification de textes : donn√©es d'entrainement annot√©es n√©cessaires


### 2018 DatalessTextClassificationATopicModelingApproachwith DocumentManifold [En cours]
**Probl√®me**: classification sans donn√©e annot√©e

**D√©finitions**
* **Collecteur de documents** (*document manifold*): structure du voisinage local

**Existant**
* PRINCIPE : Entrainement bas√© sur les mots semences
* LIMITES : possibilit√© d'information supervis√©e **limit√©e** et **bruit√©e**

**Solution**
* HYPOTHESE:
  * Les docs tr√®s similaires tendent √† faire partie de la m√™me cat√©gorie
  * En pr√©servant la structure de voisinage local, les documents d'entrainement peuvent √™tre reli√©s de tel sorte que √ßa propage la supervision m√™me si plusieurs d'entre eux ne contiennent pas de mots semences, et √ßa modifie simultan√©ment l'info d'annotation bruit√©e uniquement pour les docs contenant des mots semences non pertinents.


### 2018 - [GOOD] web - How to Build Your Own Text Classification Model Without Any Training Data [FINI][MISE EN PRATIQUE INTERROMPUE]

**L'ARTICLE PROPOSE LE WEB SERVICE Custom Classifier SUR UNE API NOMMEE [ParallelDots API](https://www.paralleldots.com/text-analysis-apis) POUR DEVELOPPER UN CLASSIFIEUR A APPRENTISSAGE ZERO SUR DES CATEGORIES PERSONELLES**

**LE SERVICE Custom Classifier EST DEPRECIE ET EST EN COURS DE REDEVELOPPEMENT (juillet 2020) DONC IMPOSSIBLE D'ALLER PLUS LOIN POUR L'INSTANT**

**D√©finitions**
* **apprentissage z√©ro** (*zero-shot learning*) : c'est √™tre capable de r√©soudre une t√¢che malgr√© n'avoir re√ßu aucun exemple de cette t√¢che.

**Avantage du type de probl√®me**
* r√©duction du cout et du temps n√©cessaire pour construire un mod√®le de classification

**Solution**
* FEATURES :
  * cat√©gories retourn√©es chacune associ√©e de son score de probabilit√©
  * le service web parvient √† retrouver avec de bons scores les cat√©gories relatives √† la requ√™te m√™me avec une division fine (les championnats europ√©ens de foot)
* SETUP :
  * [Cr√©er un compte gratuit](https://user.apis.paralleldots.com/signing-up?utm_source=website&utm_medium=footer&utm_campaign=signup) ParallelDots API et se connecter
  * le plan gratuit est mensuellement limit√© √† **1000 requ√™tes / jour** √† raison de **20 requ√™tes par minute**
  * le plan gratuit se renouvelle chaque mois
  * **LE SERVICE Custom Classifier EST DEPRECIE ET EST EN COURS DE REDEVELOPPEMENT (juillet 2020) DONC IMPOSSIBLE D'ALLER PLUS LOIN**


### 2019 Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings [FINI][A RESUMER]

**solution**
*

### 2020 Early Forecasting of Text Classification Accuracyand F-Measure with Active Learning [EN COURS]

**D√©finitions**
* **Apprentissage actif**: un apprenant s√©lectionne les donn√©es qui seront annot√©es avec pour but d'optimiser l'efficience de l'apprentissage (plus de performance avec peu de donn√©es) en demandant des efforts d'annotation o√π il devrait √™tre tr√®s utile.
* **TPC** (*Training Percent Cutoff*): proportion minimale suffisante de donn√©es d'entrainement
* **closest-to-hyperplane selection algorithm** : on s√©lectionne les *n* √©chantillons qui sont les plus proches de la fronti√®re de d√©cision (l'hyperplan)
* **mod√®le de r√©gression de courbe d'apprentissage** : calcul la performance y (erreur, accuracy, f1, etc.) en fonction du pourcentage ou de la quantit√© x de donn√©es d'entr√©e, afin de pr√©voir/anticiper le x=TPC √† partir duquel il ne sera plus n√©cessaire d'augmenter les donn√©es d'entrainement (plateau de performance):
  * lin√©aire : y = ax+b
  * logarithmique : y = a log(x) + b
  * exponential : y = ba^x
  * power law : y = bx^a

**Existant**
* **probl√®me**: l'annotation de donn√©es d'entrainement, un goulot d'√©tranglement pour la classif de texte
* **solution**: minimiser le co√ªt d'annotation avec l'apprentissage actif utilisant des m√©thodes d'arr√™t (lorsque plus d'annotation n'est plus n√©cessaire)
  * **conditions d'utilit√© des mth d'arr√™t** : pr√©voir efficacement la performance des mod√®les de classif de textes
  * **Comment**: utilisation de mod√®les logarithmiques r√©gress√©s sur une portion des donn√©es pendant que l'apprentissage progresse
  * **Question**: QUELLE PORTION (QUANTITE) DE DONNEES EST NECESSAIRE POUR UNE PREVISION PRECISE ? i.e. PEU => *pr√©vision t√¥t*  ou BEAUCOUP => *pr√©vision pr√©cise* ?
  * cette question est encore plus importante en apprentissage actif

**Question de recherche**
* Quelle est la diff√©rence dans la pr√©vision du nombre de donn√©es d'entrainement n√©cessaire ? Algo ? Accurracy vs F1 ?

**Conclusions**
* la F1 est plus difficile √† pr√©voir
* pr√©vision facile pour les arbres de d√©cision, moyen pour les SVM, plus difficile pour le r√©seaux de neurones.
* le logarithme de la m√©trique de performance est le meilleur mod√®le de pr√©vision

**Mes questions**
* Anticiper le nombre de donn√©es annot√©es est compr√©hensible; √ßa permet de savoir combien de donn√©es annot√©es sont n√©cessaire pour atteindre une certaine performance. Mais je ne vois pas en quoi c'est utile de pr√©voir le pourcentage de donn√©es ?

**Voir aussi**:
* 2018 ImpactofBatchSizeonStoppingActiveLearningforTextClassification
* 2018 SVMActiveLearningAlgorithmswithQuery-by-CommitteeVersusClosest-to-HyperplaneSelection

### 2015 slides - Text Classification without Supervision - Incorporating World Knowledge and Domain Adaptation [EN COURS]

**Existant**:
* les d√©fis de la cat√©gorisation de textes [**en production**]
  * Annotation par des expert du domaine pour des probl√®mes de grande taille
  * Domaines et t√¢ches divers : th√®mes, langages, etc.
  * des textes court et bruit√©s: tweets, requ√™tes, etc.
* Approches traditionnelles : adaptation au domaine cible (i.e. s√©mi-supervision, transfer learning et zero-shot learning )
  *  mais difficile de d√©terminer quel est le domaine cible ? e.g. distinguer le *sport* du *divertissement*

**Solution propos√©e 2008 & 2014**:
* **apprentissage activ√© par les connaissances** au travers de millions d'entit√©s et concepts, de milliards de relations
  * Wikipedia, freebase, Yago, ProBase, DBpedia
* Hypoth√®se : **les labels portent beaucoup d'information** (**ET NOUS AVONS EN PLUS DES DESCRIPTIONS**)
* Solution 1:
  1. gr√¢ce aux connaissances du domaine, repr√©senter les labels et documents dans le m√™me espace
  2. calculer les similarit√©s entre document et label
* choisir les labels

**Difficult√©s li√©es √† l'utilisation des connaissances**
* **phase apprentissage** :  Monter en charge, adaptation au domaine, classes en domaine ouvert ==> **pr√©senter quelques exemples int√©ressants
* **phase d'inf√©rence** sp√©cification des connaissances; d√©sambigu√Øsation,  
  * Utilisation de la similarit√© cosinus, Average (Best toujurs), Max matching, Hungarian matching (plus on a de concept, mieux il est)
* **phase de repr√©sentation**: repr√©sentation des donn√©es diff√©rentes de celle des connaissances ==> **comparer diff√©rentes repr√©sentations**
  * polys√©mie et synonymie

**Voir aussi**:
* ./2014 On Dataless Hierarchical Text Classification
* ./2008 Importance of Semantic Representation - Dataless Classification
* ./2014 Transfer Understanding from Head Queries to Tail Queries [TO READ]
* ./2015 Open Domain Short Text Conceptualization [TO READ]

### 2014 Transfer Understanding from Head Queries to Tail Queries [EN COURS]

* En recherche d'info, le plus grand d√©fi r√©side dans la gestion des **requ√™tes de "queue"**
* **requ√™te de "queue"** : requ√™te qui survient tr√®s rarement (**REFERENCE POTENTIELLE AUX MOTIFS RAREMENT SOLLICITES DANS NOTRE CAS => FAIBLEMENT REPRESENTEES DANS LES LOGS => DIFFICILE DE LES APPRENDRE PAR DES ALGORITHME D'ORDONNEMENT**)
* Les **requ√™tes de "t√™te"** sont facile √† g√©rer car leurs intentions sont mises en √©vidence par le grand nombre de "donn√©es clic" (i.e. **de sollicitation**)
* Le probl√®me est de savoir *COMMENT MIEUX ESTIMER LES INTENTIONS D'UNE REQUETE*
* LITTERATURE : **la pertinence d'une url pour une requ√™te q est estim√©e par la similarit√© moyenne entre elle et les anciennes requ√™tes q_i pond√©r√©e par le nombre de clics correspondants sur cette url lorsqu'elles ont √©t√© soumises**
  * PB : requete = **texte court** ==> insuffisance d'info contextuelle pour comparer la s√©mantique de 2 textes
  * LIMITE les ajustements avec la mod√©lisation th√©matique ou le DNN extrait la s√©mantique latente ou hi√©rarchique des requ√™tes mais sont lentes √† entrainer et √† tester
* HYPOTHESE : **il est beaucoup plus utile de consid√©rer ensemble la s√©mantique sur les anciennes requ√™tes et les clics d'utilisateurs, pour relier des requ√™tes diff√©rentes √† la surface (lexique).**
* CONDITION & DEFI : **parvenir automatiquement** √† correctement segmenter les requ√™tes en sous-expressions et identifier leurs concepts

### 2019 - slide - NLP from scratch - Solving the cold start problem for NLP [FINI]

**CETTE PRESENTATION RESTREINT LE PB DE COLD START A L'ANNOTATION FACILE ET RAPIDE D'UN JEU DE DONNEES PERTINENT POUR ENTRAINER UN MODELE DE MACHINE LEARNING LE PLUS EFFICACEMENT POSSIBLE. ELLE EXPLORE AINSI L'APPRENTISSAGE FAIBLEMENT SUPERVISE QUI PEUT ETRE AFFINER EN INCLUANT L'HUMAIN DANS LE PROCESSUS D'ANNOTATION PAR L'ACTIVE LEARNING. POUR CET AFFINEMENT, DEUX TECHNIQUES DE SELECTION DE DOCUMENTS PERTINENTS A ANNOTER SONT MISES EN AVANT : l'√©chantillonnage contradictoire ET l'√©chantillonnage de coude. LA PRESENTATION PROPOSE AUSSI LE TRANSFERT D'APPRENTISSAGE POUR BENEFICIER DE MODELES PREENTRAINES SUR D'AUTRES DONNEES.**

**D√©finition**
* Le **probl√®me du cold start** consiste √† trouver un moyen aussi efficace que possible pour apprendre √† r√©soudre un probl√®me d'apprentissage automatique lorsqu'on ne dispose pas de donn√©es annot√©es d'entrainement d√®s le d√©part.

**Approches existantes**
* **Supervision faible** : si on dispose de donn√©es non annot√©es, on peut annoter des cas faciles automatiquement et rapidement en d√©finissant un algorithme √† base de r√®gles ou d'heuristiques. Malgr√© les quelques erreurs d'annotation r√©sultantes, on peut utiliser ces donn√©es annot√©es pour entrainer un mod√®le de machine learning. Les r√®gles sont faciles √† √©crire et apportent une haute pr√©cision mais sont rigides. par contre le machine learning est g√©n√©ralisable aux divers variantes et probabiliste (gestion des incertitudes et de l'al√©atoire) mais n√©cessite des donn√©es annot√©es.
* **Apprentissage actif** : l'annotation des donn√©es d'entrainement peut-√™tre plus pr√©cise en s√©lectionnant les √©l√©ments les plus pertinents (???) et en les passant √† un oracle (humain de pr√©f√©rence) pour annotation manuelle ou validation d'annotation automatique. La s√©lection des documents pertinents peut se faire par l'**√©chantillonnage contradictoire**  ou par l'**√©chantillonnage de coude** (2x plus rapide)
* **Apprentissage par transfert** :
  * la technique simple consiste √† utiliser des mod√®les de vecteurs de mots pr√©entrain√©s (transfer 1.0)
  * il est possible d'affiner le mod√®le de langue sur des donn√©es du domaine cible (transfer 2.0)

### 2019_ESWC_KB_Short_Text_Classification_Using_Entity_and_Category_Embedding [FINI]

**CET ARTICLE VEND UNE APPROCHE QUI PERMET DE SE PRIVER D'ANNOTATION DE DONNEES D'ENTRAINEMENT GRACE A UNE KB, MAIS ELLE NE MARCHE QUE SI LES CATEGORIES DE LA TACHE CORRESPONDENT A DES CATEGORIES DE WIKIPEDIA ==> CE QUI REND DIFFICILE LA GENERALISABILITE DE LA METHODE**

Ce travail propose d'adresser l'absence de donn√©es annot√©es. La technique consiste √† utiliser la similarit√© "s√©mantique" entre cat√©gories et textes en s'appuyant sur des techniques de **graph embedding**.

**D√©finitions et notation**
* t : texte √† classifier
* **entit√© e** : un lien hypertexte de Wikipedia index√© par un texte de lien dans le dico pr√©fabriqu√© **Anchor-Text Dictionary** (e)
* **mention m_e** : un terme dans le texte t qui peut se ref√©rer √† e
* **contexte C_e** de e : ensemble d'autres mentions dans t except√©e celle de e
* E_t : ensemble de toutes les entit√©s possibles contenues dans t
* **Popularit√© P(e) de E** : Probabilit√© qu'un texte pris au hasard contienne e
* N : nombre d'entit√©s dans le dico
* **Relation entit√©-cat√©gorie P(c|e)**
* **e est directement associ√©e √† la cat√©gorie c, not√©e c_{a_e}** : e apparait dans un article de Wikipedia qui a comme cat√©gorie associ√©e c_{a_e}
* C_{a_e} : ensemble des cat√©gories directement associ√©es √† e
* sim(c,e) : similarit√© cosinus entre les vecteurs de c et e dans l'espace d'embedding
* A_{c_{a_e}} : ensemble des anc√™tres de c_{a_e} dans la structure hi√©rarchicale des cat√©gories dans la KB (Wikipedia)
* **Association mention-entit√© P(m_e|e)** : probabilit√© d'observer une mention m_e √©tant donn√©e l'entit√© e
* count(m_e, e) : nombre de lien utilisant m_e comme texte de lien pointant sur e comme destination
* M_e : ensemble de toutes les mentions qui peuvent se r√©f√©rer √† e (qui pointent vers e dans les articles de Wikip√©dia)
* **Relation entit√©-contexte P(C_e|e)** :
* e_c \in C_e : entit√© √† laquelle se r√©f√®re une mention du contexte C_e de e
* E_{C_e}} ensemble des entit√©s qui peuvent √™tre r√©f√©renc√©es par les mections de C_e.


**Principe (application)**:
1. D√©tection des mentions d'**entit√©s** du texte en entr√©e : n-grammes qui matchent une entr√©e d'un dico (**Anchor-Text Dictionary**)
2. G√©n√©ration, pour chaque mention, d'un ensemble d'**entit√©s** candidates √† l'aide du dico pr√©fabriqu√© (**Anchor-Text Dictionary**)  
  * PETITE IDEE: *indexer le dico comme la librairie [SML](http://www.semantic-measures-library.org) [Harispe et al. , 2013] indexe les mots dans des fichiers multiples: un chunkfile pour les n-grammes d√©butant par les m√™mes 2 premiers chars particulier, indexer les noms de fichiers de  d√©buts de n-grammes dans un fichier chunk_index)*
3. Application de la **m√©thode probabiliste** propos√©e (similaire √† un classifieur bay√©sien P(c|t) ~= P(c,t) = P(c)P(t|c) pour trouver la cat√©gorie la plus pertinente s√©mantiquement pour le texte. **Comment calculer P(c,t) ?** :
  * en utilisant les embeddings d'entit√©s et de cat√©gories appris √† partir de Wikipedia  
  * P(c,t) = \sum_{e in E_t} P(e)P(c|e)P(m_e|e)P(C_e|e)
  * P(e) = \frac{1}{N} *entit√©s √©quiprobables* ou encore *distribution uniforme*
  * P(c|e) :  si c est un c_{a_e} alors P(c|e) = P(c_{a_e}) = \frac{sim(c_{a_e}, e)}{\sum_{c'_{a_e} \in C_{a_e}} sim(c'_{a_e}, e)} sinon P(c|e) = \sum_{c_{a_e} \in C_{a_e}} P(c_{a_e}|e) P(c|c_{a_e})
  * P(c|c_{a_e}) = si c \in A_{c_{a_e}} alors \frac{1}{|A_{c_{a_e}}|} sinon 0
  * P(m_e|e) = \frac{count(m_e, e)}{\sum_{m'_e \in M_e} count(m'_e, e)}
  * P(C_e|e) = \sum_{e_c \in E_{C_e}} P(e_c|e)P(m_{e_c}|e_c)
  * P(e_c | e) = \frac{sim(e_c,e)}{\sum_{e' \in E} sim(e',e)}

**Embedding d'entit√©s et cat√©gories, pour calculer les similarit√©s**
*  construction des r√©seaux de co-occurrence
  *  entit√©-entit√© : poids = nombre de fois que 2 entit√©s apparaissent dans le m√™me article comme anchor-text (**graphe homog√®ne**)
  * entit√©-categorie : poids = nombre de fois que l'entit√© pointe sur un lien dans un article class√©e dans la cat√©gorie (bas de l'artcle) (**graphe h√©t√©rog√®ne**)
*  Mod√®le d'embedding
  * objectif : **capturer la proximit√© de second-ordre** : calcul√© entre 2 sommets en consid√©rant leur sommets partag√©s (voisins) : plus on partage de voisins plus on devrait √™tre proche
  * C'est traduit par la proba conditionel P(v_j|v_i) = \frac{exp(-u_j^T * u_i)}{\sum_{v_k \in V} exp(-u_k^T * u_i)} ~= \frac{w_{ij}}{d_i}
  * V : ensemble des sommets connect√©s avec v_i
  * u_i : vecteur du sommet v_i
  * w_{ij} : poids de l'ar√™te entre v_i et v_j
  * d_i : degr√© sortant de v_i
  * en se basant sur la divergence KL, il faut minimiser O_{homo} = - \sum_{(v_i, v_j) \in E} w_{ij} log(p(v_j | v_i)) => graphe entit√©-entit√©
  * Pour apprendre en m√™me temps sur le graphe heterog√®ne: O_{heter} = O_{ee} + O_{ec}


**Pr√©paration (apprentissage)**
1. **Pr√©fabriquation du Anchor-Text Dictionary** :
  * tous les **anchor text** (texte cliquable d'un lien hypertexte ; dit aussi *label de lien*, ou *texte de lien*) sont recup√©r√©s
  * le texte de lien est une mention utilis√© comme **cl√©** dans le dico, et le lien qu'il r√©f√©rence est l'**entit√©**

**Questions**
  * Le dictionnaire est-il construit sur le Wikipedia limit√© √† la langue des textes (e.g. seulement l'anglais) ?
  * Pourquoi la popularit√© de e n'est pas estim√©e √† partir de wikip√©dia (ou le dico) comme l'association mention-entit√© ?

**Voir aussi**:
  * dataless intent recognition/2019 Knowledge-Based Dataless Text Categorization
  * dataless intent recognition/2019 Knowledge-Based Short Text Categorization Using Entityand Category Embedding
  * 2018 TECNE - Knowledge Based Text ClassificationUsing Network Embeddings
  * Pour la m√©thode de minimisation de O_{heter}, 2015 PTE-PredictiveTextEmbeddingthroughLarge-scaleHeterogeneousTextNetworks


### 2020 Description Based Text Classification with Reinforcement Learning [MIS DE COTE]
**L'article n'est pas dataless, il utilise la description pour retrouver le passage pertinent √† classifier c'est tout. le mod√®le reste gourmand en donn√©es annot√©es**

**Nom de la m√©thode** : SQuAD-style machine reading comprehension task

Ce papier d√©crit la classification de texte sans donn√©es annot√©es comme prenant *en entr√©e* la **description de la cat√©gorie** et le **texte**, pour d√©terminer si le texte est de la cat√©gorie.
Plusieurs th√®mes apparaissent dans un document, aussi bien que plusieurs sentiments sur diff√©rents aspects. La phlosophie des auteurs est que **le mod√®le doit apprendre √† associer le texte pertinent √† l'aspect cibl√©, et ensuite d√©cider du sentiment**. L'association est formalis√©e et permet de dire **explicitement** au mod√®le ce qu'il doit classifier.


### 2018 A Pseudo Label based Dataless NB Bayes Algorithm for Text Classification with Seed Words [EN COURS]

Les auteurs soutiennent que la production de donn√©es annot√©es est trop exigente pour l'effort humain m√™me en petite quantit√©. L'approche propos√©e se base sur une t√¢che qu'ils estiment plus facile, la proposition de **mots "semences"** repr√©sentatifs des labels. L'algorithme apprend directement √† partir des documents non annot√©s et des mots "semences".

**Nom de la m√©thode** : PL-DNB (*Pseudo-Label based dataless Naive Bayes classifier*)

**Donn√©es dispo au d√©part**:
* S^L : mots cl√©s s√©lectionn√©s manuellement √† partir des labels de cat√©gories (1 seul en moyenne)
* S^D : mots cl√©s s√©lectionn√©s manuellement par expertise de domaine √† partir d'une liste produite automatiquement (non supervis√©e)
* D_U : ensemble de documents non annot√©s

**Application : Bay√©sien Na√Øf s√©mi-supervis√©**
* **s√©mi-supervis√©** : appris √† la fois √† partir de donn√©es annot√©es et non annot√©es
* y = \max_{c_i \in C}P(y = c_i|d)
* P(y = c_i|d) ~= P(y = c_i)\prod_{j=1}^{||W} P(w_j|y = c_i)^{N_{d,w_j}}
* N_{d,w_j}: nb occurrence de w_j dans d

**Entra√Ænement**:
L'entra√Ænement est une boucle de g√©n√©ration ou m√†j d'annotations et d'estimation des param√®tres du classifieur bay√©sien s√©mi-supervis√©.
1. **g√©n√©ration et m√†j d'annotations de donn√©es annot√©es D_L**
  * *Initialisation*

2. **Estimation des param√®tres \theta={P(y), P(w_j|y)}) du Bay√©sien Na√Øf s√©mi-supervis√© : Expectation-Maximisation (EM)**


### medium_com_ai_medecindirect_unsupervised_text_classification [WEB][FINI]

**D√©finitions**
*  **StackedEmbeddings** (vecteurs empil√©s) : concat√©nation des embeddings d'un mot obtenus √† partir de diff√©rentes techniques (Glove, w2v, Elmo, BERT, fastText, etc.)

**Hypoth√®se** : apprendre d'une description est suffisant pour un homme et devrait l'√™tre aussi pour la machine. On ne devrait pas forc√©ment n√©cessiter une grande masse de donn√©es annot√©es pour qu'un syst√®me reconnaisse un concept.

**T√¢che** : classer les feedbacks de clients de M√©decinDirect dans 11 cat√©gories pr√©d√©finies (indiquant probablement la raison de l'insatisfaction ou du commentaire du client)

**Probl√®me** : tr√®s faible quantit√© de donn√©es annot√©es, tr√®s d√©s√©quilibr√©es (une classe avec seulement 3 exemples sur 200 pour 11 cat√©gories). Par cons√©quent, la configuration n'est pas bonne pour de la classificaton supervis√©e traditionnelle.

**Approche** :

*  utiliser les embeddings de mots et construire le vecteur vecteurs empil√©s d'embeddings pour une phrase (requ√™te : CommentVector) ou un document (description de cat√©gorie faite de 3 phrases synth√©tiques exemples : CategoryVector) (documentEmbeddings).
*  calculer la similarit√© cosinus entre les vecteurs de requ√™tes et ceux de la cat√©gorie
*  la cat√©gorie ayant le score de similarit√© le plus √©lev√© est retenu pour la requ√™te, s'il n'y en a pas, alors la cat√©gorie de la requ√™te est d√©finie comme "incertaine" (un label en plus).
*  Utiliser les exemples annot√©es pour d√©terminer les hyperparam√®tres : type de documentEmbeddings, le type de description √† adopter pour les labels, et √† quel point le model doit √™tre, etc.



## dataless annotation/data augmentation

### 2020 When does data augmentation help generalization in NLP [EN COURS][PRIORITAIRE]
**Probl√®me**
* les r√©seaux de neurones apprennent des "faibles" features
* l'augmentation de donn√©es d'entrainement encourage les algos √† pr√©f√©rer les features "fortes"
* Dans quelles conditions l'augmentation des donn√©es d'entrainement √©vite l'utilisation des features faibles?
  * Combien de contreexemples doivent √™tre vus pour √©viter une faible feature donn√©e?
  * Est-ce que ce nombre croit avec la taille originale des donn√©es d'entrainement ?
* Dans quelles conditions l'augmentation des donn√©es d'entrainement n'encourage pas l'utilisation des features plus forts?
  * Est-ce que la relative difficult√© de repr√©senter une feature impact son adoption par le mod√®le?
  * Comment l'effectivit√© de l'augmentation des donn√©es change dans les config qui contiennent plusieurs faibles features main une seule feature forte?
* ces probl√®mes sont relatives √† la perturbation de la distribution de l'entrainement, la g√©n√©ralisation interdomaine, les donn√©es d'entrainement non-iid (La plupart des m√©thodes de conception de classificateurs supposent que les √©chantillons d'apprentissage sont tir√©s ind√©pendamment et de mani√®re identique d'une distribution de g√©n√©ration de donn√©es inconnue, bien que cette hypoth√®se soit viol√©e dans plusieurs probl√®mes de la vie r√©elle.[Dundar et al., IJCAI07, Learning Classifiers When The Training Data Is Not IID])


### 2019 EDA - Easy Data Augment Tech for Boosting Perf on Text Classif [En cours][PRIORITAIRE]8
**Probl√®me**: insufisance des donn√©es annot√©es pour l'entrainement d'un classifieur de textes

**Solution**: g√©n√©rer "facilement" de nouvelles donn√©es annot√©es √† l'aide de 4 op√©rations:
* remplacement par synonyme : remplacer n mots (non stopword) choisis al√©atoirement par leur synonyme
* insertion al√©atoire : n fois, remplacer un mot (non stopword) al√©atoirement choisi par un synonyme
* permutation al√©atoire : n fois, permuter la position de 2 mots choisis al√©atoirement
* suppression al√©atoire : supprimer al√©atoirement chaque mot dans la phrase

**Code**: https://github.com/jasonwei20/eda_nlp

**voir aussi**
* https://towardsdatascience.com/text-data-augmentation-makes-your-model-stronger-7232bd23704
* https://medium.com/opla/text-augmentation-for-machine-learning-tasks-how-to-grow-your-text-dataset-for-classification-38a9a207f88d
* https://towardsdatascience.com/data-augmentation-for-text-data-obtain-more-data-faster-525f7957acc9
* https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2?gi=16ddf2ae2aa6


## dataless annotation/Weak supervision

### 2019 A clinical text classification paradigm using weak supervision and deep representation [FINI]
L'article pr√©sente une m√©thode simple pour adresser l'absence de donn√©es annot√©es pour une t√¢che de classification de textes de rapports cliniques. Il propose de construire une base d'entra√Ænement √† l'aide d'une m√©thode √† base de REGEX. Le principe est de conclure en l'appartenance d'un document √† une cat√©gorie sir une phrase de ce doc comprend un mot-cl√© ou une combinaison de mots-cl√©s pr√©d√©finis pour cette cat√©gorie (e.g. **uses tobacco** pour la classe **smoker**). Le jeu d'entra√Ænement est donc potentiellement bruit√© et ne couvre que les motifs de mots-cl√©s pr√©d√©finis. Pour √™tre robuste aux mots-cl√©s inconnus, le mod√®le vectoriel emploi les embeddings de mots qui rapprochent les mots inconnus des connus. En effet, le vecteur d'un texte est la moyenne des embeddings des mots qu'il comprend (occurrence ou **type** (le doc est d√©fini comme un ensemble de mots)?). Les exp√©rimentations montrent de tr√®s bonnes performances en classification binaire avec le CNN (F1 √† 0.92 & 0.97  pour 0.91 & 0.93 pour les r√®gles) avec la quantit√© de donn√©es d'entra√Ænement disponible (31861  et 22471) mais moins bonnes (0.77 pour 0.88 pour les r√®gles) en multi-classe (5 classes) pour deux raisons : (1) faible quantit√© de donn√©es (389), (2) un important d√©s√©quilibre du jeu annot√© (deux classes couvrant seuelement 5%).
* **Atout** :
  * disponibilit√© de donn√©es non annot√©es
  * disponibilit√© de l'expertise pour pr√©d√©finir les motifs de mots-cl√©s pour les r√®gles
* **Manque**:
  * quantit√© suffisante de textes √† annoter pour le deep learning (CNN)
*  **Avantage**:
  * l'augmentation de donn√©es annot√©es am√©liorera les performances de classification


## embeddings

### 2014 GloVe - Global Vectors for Word Representation

**voir aussi**
* homepage https://nlp.stanford.edu/projects/glove/
* code original en C/C++ : https://github.com/stanfordnlp/GloVe
* *Possible impl√©mentation fonctionnant sur windows:
    * les 2 derniers commentaires de golslan : https://groups.google.com/forum/#!topic/globalvectors/rqPmTBqFbCQ
    * code py2 : https://github.com/hans/glove.py
    * code py3 : https://github.com/maierhofert/glove.py
* http://www.foldl.me/2014/glove-python/
* code python 3 : https://github.com/maierhofert/glove.py.git
* explication : https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010

### 2017 SIF-a_simple_but_tough_to_beat_baseline_for_sentence_embeddings [EN COURS]

**Probl√®me** : comment obtenir des vecteurs qui captent suffisamment la s√©mantique des phrases √† partir de vecteurs de leurs mots?

**Existant**
* Approches non supervis√©es :
  * moyenne pond√©r√©e (tfidf-GloVe) ou pas (avg-Glove) de vecteur de mots
  * skip-thought vectors : bas√© sur le mod√®le encodeur-d√©codeur avec un encodeur pour l'entr√©e (phrase) et deux d√©codeurs pour pr√©dire respectivement la phrase pr√©c√©dente et la suivante.
* Approches semi supervis√©es :
  * avg-PSL : vecteurs de mots PARAGRAMSL999 appris par supervision, puis moyenne simple
* Approches supervis√©es :
  * PP, PP-proj: moyenne des vecteurs de mots PARAGRAMSL999 avec ou sans projection lin√©aire.
  * DAN : r√©seau profond moyennant
  * RNN, iRNN : r√©seau de neurones r√©current sans ou avec l'activation comme identit√© (??)
  * LSTM, LSTM : avec (LSTM(o.g.)) ou sans (LSTM (no)) portes de sorties

**Hypoth√®ses**
* mod√®le g√©n√©ratif √† variable latente
* [Ancien mod√®le] processus dynamique de g√©n√©ration de corpus :
  * g√©n√©ration du t √®me mot √† l'√©tape t
  * processus guid√© par une marche al√©atoire de variables latentes: vecteur de discours c_t \in R^d, vecteurs des mots w (v_w) dans R^d
  * le produit scalaire de c_t et v_w capte les correlations entre le discours et le mot w
  * proba d'observer w √† l'√©tape t : P(w √©mis √† t | c_t) ~= exp(<c_t, v_w>)
  * marche al√©atoire lente de c_t : c_t+1 obtenu de c_t en ajoutant un petit vecteur de d√©placement
  * c_t peut √™tre estim√© (c_s) pour le discours (phrase) : MAP estimate = moyenne pond√©r√©e des vecteurs de mots dans la phrase
* [Am√©lioration de la marche al√©atoire] **lissage de l'estimation de c_t**: meilleur terme de pond√©ration avec 2 types de "terme de lissage" qui doivent tenir compte du fait que certains mots apparaissent en dehors du contexte, et que certains mots courant apparaissent ind√©pendamment du discours ("the", "and", etc.):
  * introduction de \alpha*p(w), o√π p(w) est la proba d'un mot (unigram) dans tout le corpus, \alpha est un scalaire: pour permettre au mot d'apparaitre m√™me si sont produit avec c_s est tr√®s faible (mot hors contexte)  
  * introduction d'un vecteur de discours commun c_0 \in R^d : terme de correction pour les mots les plus fr√©quents (souvent li√© √† la syntaxe)
* Ainsi P(w √©mis √† t | c_t) ~= \alpha*p(w) + (1-\alpha)exp(<~c_s, v_w>)/Z_{~c_s} o√π ~c_s = \beta*c_0+(1-\beta)*c_s, c_0 et c_s √©tant orthogonaux, \alpha et \beta sont des hyperparam√®tres, Z_{~c_s} = \sum_{w \in V} exp(<~c_s, v_w>)
  * si \alpha = \beta = 0, on retombe sur le mod√®le pr√©c√©dent
  * si \alpha > 0, tout mot peut apparaitre dans la phrase (gr√¢ce √† p(w))
  * si \beta > 0, les mots corr√©l√©s avec c_0 peuvent apparaitre

**Algorithme de la m√©thode SIF** (*smooth inverse frequency*)
* calcul de la moyenne pond√©r√©e des vecteurs de mots w dans la phrase ; le poids √©tant a/(a+p(w)) avec a=param√®tre (usually set to 1e-3) et p(w) fr√©quence de w
* retrait des projections des vecteurs moyens sur leur 1er vecteur singulier (√©limination du composant commun) [re-pond√©ration pour √©viter de tr√®s grandes diff√©rences inutiles d'√©chelles entre les composantes d√ªe √† la trop grande fr√©quence de certains mots]

**Questions:**
* A quel point le SIF est sensible √† la valeur de a, p(w), et aux word embedding?
* comment employer le coefficient de correlation de pearson pour estimer le bon a?
* comment employer le coefficient de correlation de pearson pour evaluer une t√¢che de similarit√© comme dans l'aricle ( Pearson‚Äôs r √ó 100 )?
* **Est-ce important d'√©liminer les stop-words ?**
  * [R√©ponse sur BERT (de l'auteur)] l'√©limination des stopwords fait am√©liore les performances de 10% https://github.com/huggingface/transformers/issues/876#issuecomment-523228498
* **Si oui quand doit-on √©liminer les stopwords : avant l'entrainement de vecteurs de mots ou apr√®s i.e. au moment l'agr√©gation ?**
* **Est-ce qu'un entrainement pr√©alable des vecteurs de mots sur des textes du domaine cible (retail, finance, public service, health) peut am√©liorer encore plus les r√©sultats ?**

**Voir aussi**
  * https://www.offconvex.org/2018/06/17/textembeddings/
  * https://blog.dataiku.com/how-deep-does-your-sentence-embedding-model-need-to-be
  * une impl√©mentation https://www.kaggle.com/procode/sif-embeddings-got-69-accuracy
  * critique des fondements th√©oriques: https://www.groundai.com/project/a-critique-of-the-smooth-inverse-frequency-sentence-embeddings/1

### 2018-sent2vec
* m√©thode non supervis√©e d'apprentissage de la repr√©sentation vectorielle des textes: sorte d'extension de la fonction objectif de C-BOW mais pour entrainer les vecteurs de phrases
* forme g√©n√©rale : min_{U,V} \sum_{S \in C} f_S(UV i_S)
  * U \in R^kxh, et V \in R^hx|Vocab| : matrices des param√®tres
  * les colonnes de V collecte les vecteurs de mots de dimension h
  * le vecteur indicateur i_S \in {0, 1}^|vocab| est un vecteur binaire encodant S (S est la fen√™tre de contexte)
  * k = |vocab|
* le principe est :
  * d'apprendre les embeddings source v_w et destination u_w pour chaque mot w.
  * l'embedding de la phrase est d√©fini comme la moyenne des embeddings source de ces mots constituants
  * le mod√®le est augment√© en apprenant les embeddings source non seulement pour les unigrams mais aussi pour les n-grams pr√©sent dans chaque phrase :
  v_S = (1/|R(S)|) V i_{R(S)} = (1/|R(S)|) \sum_{w \in R(S)} v_w
  * R(S) √©tant la liste des n-grams pr√©sents dans la phrase S
  * afin de pr√©dire le mot manquant dans le contexte, la fonction objectif mod√©lise la sortie softmax approch√©e par √©chantillonnage n√©gatif (am√©liore le temps d'apprentissage m√™me pour un grand nombre de mots)
* la fonction objectif d'entrainement de sent2vec est : min_{U,V} \sum_{S \in C} \sum_{w_t \in S} (l(u_{w_t}.T v_{S\{w_t}}) + \sum_{w'\in N_{w_t}}l(-u_{w'}.T v_{S\{w_t}}))
  * S phrase courrante
  * l : x-> log(1+e^{-x})
  * N_{w_t} ensemble des mots n√©gativement √©chantillonn√©s pour le mot w_t de S (en suivant une distribution multinomiale o√π chaque mot est associ√© √† la proba q_n(w) = \sqrt{f_w} / (\sum_{w_i \in vocab } \sqrt{f_{w_i}} avec f_w la fr√©q normalis√©e de w dans le corpus  
  * pour s√©lectionner les possible unigrams destinations (positifs), on utilise les sous-√©chantillonnages, chaque mot √©tant √©cart√© avec la proba 1-q_p(w) o√π q_p(w) = min{1, \sqrt{t/f_w} + t/f_w}, o√π t est l'hyper-param√®tre de sous-√©chantillonnage.
  * le sous √©chantillonnage √©vite que les mots tr√®s fr√©quents n'aient trop d'influence au cours de l'apprentissage pour ne pas introduire des biais dans la t√¢che de pr√©diction
  * la fonction objectif devient : min_{U,V} \sum_{S \in C} \sum_{w_t \in S} q_p(w)(l(u_{w_t}.T v_{S\{w_t}}) + |N_{w_t}|\sum_{w'\in N_{w_t}}q_n(w')l(-u_{w'}.T v_{S\{w_t}}))

**voir aussi**
* https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/

### 2018-ImprovingLanguageUnderstandingByGenerativePreTraining-RadfordNarasimhanSalimansSutskever-GPT
**Contexte** : Compr√©hension du langage naturel (NLU), pr√©-trainement de mod√®le de langue, r√©seau de neurones profond, GPT, apprentissage semi-supervis√© pour le NLP/NLU, implication textuelle, r√©ponse aux questions, √©valuation de la similitude s√©mantique, et classification de documents

**Probl√®me** : raret√© des donn√©es annot√©es pour le NLU

**Hypoth√®ses** : Une am√©lioration consid√©rable des solutions √† ces t√¢ches peut √™tre obtenue par **pr√©-entrainement g√©n√©ratif** d'un mod√®le de langue sur un corpus diversifi√© de textes non-annot√©s, **suivi d'un affinement discriminatif** sur la t√¢che sp√©cifique

**Existant**

* Difficile d'exploiter des informations au-del√† du niveau des mots:
  1. non claret√© du type d'optimisation de fonction objectif pour apprendre les repr√©sentations des textes util au transfer de fa√ßon optimale
  2. aucun consensu sur la mani√®re la plus efficace de transf√©rer ces repr√©sentations apprises vers la t√¢che cible

* apprentissage semi-supervis√© de mod√®le de langue (LM): ajout d'une partie non supervis√©e √† la fonction objectif d'une approche supervis√©e pour exploiter une grande qt√© de donn√©es non-annot√©es dans le but d'am√©liorer les performance gr√¢ce √† une meilleur repr√©sentation des textes. OU BIEN pr√©-entrainement de la repr√©sentation des textes sur des donn√©es non-annot√©es avant de les utiliser dans un contexte supervis√©
  * l'approche propos√©e combine √† la fois une partie non-supervis√©e √† la partie supervis√©e, les 2 √©tant entrain√©s pendant l'apprentissage supervis√©  avec une **l√©g√®re modification de la partie non-supervis√©e**
* apprentissage non-supervis√© de LM: apprentissage sur des donn√©es non-annot√©es des poids de neurone du r√©seau supervis√© pour une t√¢che sp√©cifique ==> recherche non-supervis√© des valeurs d'initialisation
  * l'approche propos√©e a une partie non supervis√©e pr√©-entra√Æn√©e sur des donn√©es non-annot√©es
* apprentissage de LM auxiliaire: ajout de fonctions objectif auxiliaires (non supervis√© par rapport √† la t√¢che) par exemple des r√©seaux multi-t√¢ches (LM, POS, NER, ...)

**Solution** : objectif == apprendre une representation universelle (Transformeurs : une m√©moire plus structur√©e pour g√©rer les d√©pendances √† long terme dans le texte (compar√© au RNN)) qui se transf√®re avec une petite adaptation √† un large panel de t√¢che (traitement des entr√©es comme une s√©q continue de tokens) = faible changement sur l'architecture du mod√®le pr√©-entrain√©

**Question**
Comment utiliser cette approche sur de longs documents ? classifications de d√©cisions de justice par exemple

**voir aussi**


### 2014 doc2vec

**Existant**
* BoW  (FAIBLESSE perte d'ordre entre les mots, pas de s√©mantique)
* BoNgram : (ATOUT ordre dans le contexte) (FAIBLESSE pas de s√©mantique)
* (Bengio et al., 2006) concat√©nation des vecteurs de mots d'un contexte pour entrainer un r√©seau de neuronnes √† reconna√Ætre le prochain mot
* moyenne pond√©r√©e de tous les mots du document (FAIBLESSE perte de l'ordre comme BoW)
* (Socher et al., 2011b) combinaison des mots dans un ordre donn√© par un arbre d'analyse de structure de phrase (parsing)  d'une phrase en utilisant des op√©rations matricielles (FAIBLESSE limit√©e aux phrases parce que le parsing est utilis√©)

**Approche propos√©e : Praragraph Vector**
* ATOUT
  * applicable aux textes de toutes tailles,
  * ne n√©cessite pas d'affinement orient√© t√¢che pour la fonction de pond√©ration de mot,
  * ne d√©pend pas des arbres de structure de textes,
  * gain de 16% de taux d'erreur par rapport au SOTA pour la classification de sentiment
  * gain de 30 % par rapport au BoW pour la classification de texte

* PRINCIPE
  * MODELE : concat√©nation du vecteur du paragraphe avec ceux de plusieurs mots du paragraphe, pour pr√©dire le mot suivant
  * APPRENTISSAGE DES VECTEURS : SGD + r√©tro-propagation,
  * PREDICTION : pour pr√©dire le vecteur d'un nouveau paragraphe, l'intuition est que les paragraphes (leur vecteur) est unique, mais il partage les vecteurs de mots. le nouveau vecteur est inf√©r√© en fixant les vecteur de mots et en entra√Ænant le nouveau vecteur de paragraphe jusqu'√† la convergence

**voir aussi**
* CODE entra√Ænement sur wikipedia : https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html
* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5


### 2017 fasttext - Enriching Word Vectors with Subword Information

**voir aussi**
* https://medium.com/paper-club/bag-of-tricks-for-efficient-text-classification-818bc47e90f#:~:text=%20Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification,up%20of%20N%20ngram%20features%20in...%20More%20
* https://towardsdatascience.com/fasttext-ea9009dba0e8
* Utilisation pour la correction orthographique:
  * https://github.com/Bicky23/FastText-Spell-Checker
  * https://www.haptik.ai/tech/extract-spelling-mistakes-fasttext/

### 2020 P-SIF - Document Embeddings Using Partition Averaging [PRIORITAIRE]

### 2015 [GOOD PERF] AdaSent-SelfAdaptiveHierarchicalSentenceModel [PRIORITAIRE]
** existant**
* cBoW:
  * agr√©gation de vecteurs de mots : avg ou max
  * inconv√©nient: insensible √† l'ordre entre les mots et aussi √† la longueur de la phrase; par cons√©quent, il est possible pour 2 phrases de sens diff√©rents d'avoir la m√™me repr√©sentation vectorielle

**Principe**
* repr√©sentation hi√©rarchique de phrase (mot-terme/expression-phrase) sous forme de flow simulant √† la fois des recurrent nn et des recursive nn. chaque niveau est agr√©g√© et la pyramide enti√®re est r√©duite en une hi√©rarchie H; la hi√©rarchie est ensuite fournie √† un gating network et √† un classifier pour former un ensemble
* en recurrent nn, la dynamique consiste √† transformer cons√©cutivement les mots combiner au vecteur cach√© pr√©c√©dent; le dernier vecteur cach√© √©tant celui de la phrase (h_0 = 0  ; h_t = f(Wh_t^0+Hh_{t-1}+b). W est la matrice de connexion entr√©e-cach√©, H est la matrice de connexion r√©currente cach√©-cach√©
* en recursive nn, l'id√©e est de composer suivant un arbre binaire pr√©d√©fini dont les mots (leur vecteur) sont les feuilles. Des transformations non-lin√©aires sont r√©cursivement appliqu√©s du bas vers le haut pour g√©n√©rer la repr cach√© d'un noeud parent √† partir de la repr de ses 2 fils (h = f(W_L h_l + W_R h_r + b). W_L et W_R sont les matrices de connexion recursive cauche et droite, h_l et h_r sont les repr cach√©es des fils gauche et droit.

**Diff√©rence avec grConv (r√©seau de neuronne r√©cursif √† porte**
* AdaSent forme une hi√©rarchie d'abstractions de la phrase d'entr√©e
* AdaSent nourrit la hi√©rarchie comme un r√©sum√© dans le classifieur suivant
* combin√© √† un r√©seau de portes pour d√©cider du poids de chaque niveau dans le consensus final.

**Structure**
* graphe acyclique orient√©
* structure pyramidale de T niveaux (de 1 √† T du bas vers le haut) pour une entr√©e (phrase) de longueur T
* la port√©e de chaque unit√© due niveau t=1 est le mot correspondant i.e. scope(h_j^1) = {x_j} \forall j \ in 1:T
* \forall t>=2 scope(h_j^t) = scope(h_j^{t-1}) \cup scope(h_{j+1}^{t-1}) = {x_{j:j+t-1}}
* le niveau t contient T-t+1 unit√©s, et chaque unit√© a une port√© de taille t*
* l'unit√© h_j^t peut √™tre interpr√©t√© comme le r√©sum√© de l'expression x_{j:j+t-1} dans la phrase originale.
* le niveau 1 comprend les vecteurs de mots
* le niveau T est le r√©sum√© global de la phrase enti√®re
* **pretraitement** transformation lin√©aire des vecteurs de mots de R^d √† R^D (D >= d): la repr√©sentation cach√©e au niveau 1 est h_{1:T}^1 = U'h_{1:T}^0 = U'Ux_{1:T}, o√π U' \in R^{Dxd} est la matrice de transformation lin√©aire dans AdaSent et U \in R^{dxV} est la matrice d'embeddings de mots entrain√©s sur un large corpus non-annot√©: **cette factorisation aide √† r√©duire le nombre de param√®tre du model lorsque d << D**

**Composition locale et niveau de pooling (mise en commun)**
* la composition locale r√©cursive : h_j^t = w_l h_j^{t-1} + w_r h_{j+1}^{t-1} + w_c h~_j^t, avec  h~_j^t = f(W_L h_j^{t-1} + W_R h_{j+1}^{t-1} + b_W) avec
  * j \in 1:T-t+1,
  * t \in 2:T,
  * W_L et W_R \in R^{DxD} sont les matrices de combinaison cach√©-cach√©, matrices r√©currente doubl√©es,
  * b_W \in R^D est le vecteur biais
  * w_l, w_r, et w_c sont les coefficients de porte s.c. w_l, w_r, w_c >= 0 et w_l + w_r + w_c = 1
* h_j^t, w_l, w_r, et w_c sont des fonctions param√©tr√©es de h_j^{t-1} et h_{j+1}^{t-1} de telle sorte qu'ils peuvent d√©cider soit de composer ces enfants par une transfo non-lin√©aire ou simplement transmettre leur repr√©sentation pour de futures compositions.
* une fois la pyramide construite, on applique une agr√©gation de moyenne ou de max sur le niveau t pour obtenir un r√©sum√© \bar{h}^t de toutes les expressions cons√©cutives de taille t dans la phrase originale

**r√©seau de porte**
* l'approche peut-√™tre √©tendu pour r√©soudre un pb de classification
  * soit g() un classifieur discriminatif qui prend \bar{h}^t \ in R^D en entr√©e et retourne les proba des diff√©rentes classes.
  * w() est le r√©seau de porte qui prend \bar{h}^t en entr√©e et retourne un score de confiance 0<=\gama_t<=1
  * intuitivement \gama_t d√©peind la confiance qu'il y a dans le fait que le niveau t de r√©sum√© dans la hi√©rarchie, est ad√©quat pour √™tre utilis√© comme une repr ad√©quate pour l'instance d'entr√©e courante pour la t√¢che en main.
  * on exige que  \gama_t >= 0 et \sum_{t=1}^T \gama_t = 1
* soit C la variable al√©atoire correspondant au label de la classe,
* le consensus du syst√®me entier est atteint en prenant un m√©lange des d√©cisions faites par les niveaux de r√©sum√© de la hi√©rarchie:
  * p(C = c | x_{1:T}) = \sum_{t=1}^T p(C=c|H_x=t) p(H_x = t|x_{1:T}) = \sum_{t=1}^T h(\bar{h}^t) w(\bar{h}^t)

**retro-propagation √† travers la structure (BPTS)**
* la BPTS est utilis√© pour calculer les d√©riv√©es partielles (les matrices W_L, W_R, G_L, G_R) de la fonction objectif L() par rapport au param√®tres du model
* dL/dW_L = \sum_{t=1}^T \sum_{j=1}^{T-t+1} dL/dh_j^t dh_j^t/dW_L
* dL/dW_R = \sum_{t=1}^T \sum_{j=1}^{T-t+1} dL/dh_j^t dh_j^t/dW_R
* gr√¢ce √† la structure DAG : dL/dh_j^t = dL/dh_j^{t+1} dh_j^{t+1}/dh_j^t + dL/dh_{j-1}^{t+1} dh_{j-1}^{t+1}/dh_j^t
* les formulations locales BP: dh_{j-1}^{t+1}/dh_j^t = w_r I + w_c diag(f')W_R et dh_{j}^{t+1}/dh_j^t = w_l I + w_c diag(f')W_L
  * I matrice identit√©
  * diag(f') matrice diagonale couverte par le vecteur f' qui est la d√©riv√©e de f pr rapport √† son entr√©e

**Voir aussi**
* Impl√©mentation de l'auteur: https://hanzhaoml.github.io/papers/IJCAI2015/adasent.zip
* Impl√©mentation : https://github.com/AllenCX/Adasent-pytorch [seems the best online]
* Impl√©mentation : https://github.com/Mooonside/AdaSent [seems to have the best io]

### 2019 Bert - Pre-Training of Deep Bidirectional Transformers for Language understanding


### 2020 CamemBERT - a Tasty French Language Model
**voir aussi**
* https://blog.baamtu.com/word2vec-camembert-use-embedding-models/

### 2020 SBERT-WK - A Sentence Embedding Method By Dissecting BERT-based Word Models [PRIORITAIRE]

### 2019 Sentence-BERT [PRIORITAIRE]

### 2020 Improving Sentence Representations via Component Focusing (CF-BERT)
**existant**
* moyenne des vecteurs de mots : methode la plus facile et plus populaire
* LSTM : meilleur perf, mais complexe √† mettre en oeuvre ;
* RNN: obtienne l'info global par recursion graduelle;
* CNN: n'obtient que l'info local; utilisation de filtres convolutionnels pour capter les d√©pendances locales + application de la couche d'agr√©gation pour extraire les features globales
* Transformers: obtiennent directement l'info global, sont plus rapide car ex√©cution en parall√®le
* BERT : utilise des transformer mais aucun embedding ind√©pendant de phrase n'est calcul√©, (difficile de d√©river un vecteur de phrase de BERT);
* SIF embedding : somme des embeddings de mots pond√©r√©s par l'idf et soustrit √† un vecteur bas√© sur les composantes principale des vecteurs de phrases
* Sent2Vec apprend les features de  n-grams dans la phrase pour pr√©dire le mot central √† partir du contexte environnant
* Skip-thought : un encoder neural s√©quentiel de phrase, entraine une archi encodeur-d√©codeur qui peut pr√©dire les phrases environnantes
* InferSent : un encoder neural s√©quentiel de phrase, utilise des donn√©es lab√©lis√©es des datasets SNLI et Multi-Genre NLI pour entrainer un r√©seau BiLSTM siamois avec l'agr√©gation par max sur la sortie pour g√©n√©rer l'encodeur de phrase.
* USE universal Sentence Encoder : entraine un r√©seau de transformeur et augmente l'apprentissage non-supervis√© initialement r√©alis√© sur un corpus non annont√© comme wikip√©dia puis continu√© sur SNLI
* Sentence-BERT (SBERT) : utilise une structure de r√©seau siamois pour affiner le r√©seau pr√©-entrain√© BERT premi√®rement par NLI puis sp√©cifiquement par les t√¢ches sp√©cifique de NLP pour d√©duire des repr s√©mantiquement significative de phrases.

**proposition CF-BERT**
* modification du r√©seau pr√©-entrain√© BERT
* utilise une structure de r√©seau siamois pour d√©river des vecteurs s√©mantiques significatifs par focalisation sur les composants
* divise la repr√©sentation d'une phrase en 2 parties:
  * **partie de base** correspond √† la phrase compl√®te (positon dominante)
  * la **partie am√©lior√©e par composant** qui tient compte de l'info pertinente et r√©duit l'impact des mots nuisibles sur le sens de la phrase (r√¥le de suppl√©ment)
* Pour la partie am√©lior√©e, un arbre de d√©pendance grammaticale est utilis√©
* un facteur de poids est d√©termin√© par grid search pour g√©n√©rer la repr√©sentation optimale de la phrase
* le vecteur final est obtenu par une strat√©gie d'agr√©gation
* CF-BERT est du SBERT si le facteur poids de la partie focalis√©e composant est nul (W_{cf}=0) : emb_S = emb_{S_{cf}} * W_{cf} + emb_{S_{basic}}
* la couche de sortie peut √™tre d√©finir en fonction de la t√¢che sp√©cifique √† r√©soudre

### 2019 spherical-text-embedding
**existant**
* word2vec : apprend les repr de mots en espace euclidian en mod√©lisant les co-occurrences locales entre mots
* gap entre l'apprentissage en espace euclidian et l'usage en espace sph√©rique (normalisation pr√©alable des vect tfidf, similarit√© cosinus)


**proposition**
* apprendre directement la repr de texte en espace sph√©rique ne imposant des contraintes de norme sur les repr i.e. d√©finition d'un mod√®le g√©n√©ratif √† 2 √©tapes √† la surface d'une sph√®re unitaire
  * un mot est g√©n√©r√© en premier conformement √† la s√©mantique du paragraphe
  * ensuite, les mots environnant sont g√©n√©r√©s en consistance evec la s√©mantique du mot central
* l'apprentissage est d√©fini par une proc√©dure d'optimisation efficiente de Rieman
* Autre avantage: apprentissage joint de repr de mots et de repr de paragraphes; la repr des paragraphes peut √™tre directement obtenue pendant l'entra√Ænement grace √† la mod√©lisation explicite de la relation g√©n√©rative entre mots et leur paragraphe
  * meilleur repr de mots par exploitation jointe des stats de co-occurrence mot-mot et mot-paragraphe
* rapidit√© d√ª au remplacement de la couche conventionnelle softmax par la fontion sph√©rique de perte


**voir aussi**
* Code de l'auteur: https://github.com/yumeng5/Spherical-Text-Embedding
* slides: https://yumeng5.github.io/files/kdd20-tutorial/Part1.pdf


## knowledge graph

### 2017 Graph-based_Text_Representations_Tutorial_EMNLP_2017 [TURORIEL][FINI][A RESUMER]

**Objectifs**: Booster la fouille de textes, le TALN, et la RI avec les graphes

**Probl√®mes de la repr√©sentation par BoW**: hypoth√®se d'ind√©pendance entre termes et pond√©ration par fr√©quence de termes

**Int√©r√™t de la repr√©sentation de texte par graphe**: capturer la **d√©pendance** entre les termes, de leur **ordre** et la **distance** entre eux.


## query analysis

### 2018_Chapter_UnderstandingInformationNeeds [SUSPENDU]

en RI, la compr√©hension de l'information n√©cessit√©e par l'utilisateur passe par une bonne compr√©hension de sa requ√™te. Pour cela, il existent des techniques comme la classification de la requ√™te suivant des buts de niveau √©lev√© (intention), segmentation en parties allant ensemble (e.g. noms compos√©s), interpr√©ter la structure de la requ√™te, recona√Ætre et d√©sambigu√Øser les entit√©s mentionn√©es, d√©terminer si un service sp√©cifique ou un segment/domaine des contenus en ligne (*verticals*, e.g. shopping, voyage, recherche d'emploi, etc.) doit √™tre invoqu√©.

#### Analyse s√©mantique de requ√™te

**Classification de requ√™te**

**Annotation de requ√™te**

**Interpr√©tation de requ√™te**


## short texts similarity

### 2019 Sentence Similarity Techniques for Short vs Variable Length Textusing Word Embeddings [FINI]*
**Probl√®me**: Comment estimer la similarit√© s√©mantique entre une phrase S1 courte (1-3 mots e.g. des commandes courtes comme "*supprimer la commande*" ou "*montrer les √©l√©ments r√©cents*") et une autre phrase S2 plus grande ?

**Contexte**: agent conversationnel (e.g. chatbot)

**Existant**:
* le cosinus de similarit√© des sac-de-mots ne tient pas compte de l'ordre des mots dans la phrase
* la somme ou moyenne (pond√©r√©e ou pas) des vecteurs de mots :
  * inconv√©nient: le cosinus est invariant entre la somme et la moyenne
  * inconv√©nient: la moyenne pond√©r√©e n'aide pas pour une longue phrase
  * avantage: la somme et  la moyenne sont moins ch√®res √† calculer
* probl√®me dans le dev de chatbot:
  * n√©cessit√© d'un grand nombre de donn√©es annot√©es d'entrainement
  * gestion perp√©tuelle de plusieurs donn√©es √† chaque pr√©diction : complexe et ch√®r en temps

**Proposition1: fen√™tre coulissante avec moyenne pond√©r√©e des vecteurs de mots**
* taille_fenetre = nombre de mot de S1 (la phrase la plus courte)
* S2 est d√©coup√© en sous chaines {S2_j} de taille taille_fenetre
* le vecteur de chaque S2_i est la moyenne des vecteurs de ses mots
* sim(S1, S2) = max_j cos_sim(vecteur_S1, vecteur_S2_j)

**Proposition2: vecteurs pond√©r√©s de n-grams**
* N-gram = sous-chaines de N mots cons√©cutifs (e.g. unigram=1-gram, bigram=2-gram, trigram=3-gram)
* les phrases Si (i \in 1:2) sont d√©coup√©es en N_{i1} unigrams {Si_1-gram_j}, N_{i2} bigrams {Si_2-gram_j}, N_{i3} trigrams {Si_3-gram_j}, ..., Nmax-gram {{Si_Nmax-gram_j}}
* le vecteur de chaque N-gram est la moyenne des vecteurs de ses mots
* \forall k in 1:Nmax, score_k = 1/N_{1k} \sum_{n1 \in 1:N_{1k}} max_{n2 \in 1:N_{2k}} {cos_sim(vecteur_{S1_k-gram_j}, vecteur_{S2_k-gram_j})}
* sim(S1, S2) = \sum_{k \in 1:Nmax} w_k * score_k, avec w_k = k / {\sum_{k \in 1:Nmax}}

**discussion des r√©sultats**
* LIMITE: **dataset con√ßu par les auteurs** : une commande courte de base (S1) et diverses fa√ßons (g√©n√©ralement un peu plus longue) d'exprimer cette commande (S2)
* le seuil minimal de similarit√© est fix√© √† 0.9
* proposition1 beaucoup moins bonne que la proposition 2 (F1-score de 0.3708 contre 0.9316, et 0.1451 pour le cos_sim Google's Universal Sentence Encoding)

## short texts similarity / metric learning

### 2019 ATutorialonDistanceMetricLearning-MathematicalFoundationsAlgorithmsandExperiments [EN COURS]

*  Une distance standard peut ignorer des propri√©t√©s importantes dans le dataset = son utilisation par un apprentissage rendant ce dernier non optimal
*  L'objectif de l'apprentissage d'une distance, c'est de rapprocher autant que possible les objets similaires, tout en √©loignant les diff√©rents, pour am√©liorer la qualit√© des applications
*  Les bases de l'apprentissage de distance sont :
  *  **l'analyse convexe** : pour la pr√©sentation et la r√©solution de pbs d'optimisation (estimation de param√®tres)
  *  **l'analyse matricielle** : pour la compr√©hension de la discipline, la param√©trisation des algo, et l'optimisation par les vecteurs propres
  *  **la th√©orie de l'information** : qui a motiv√© plusieurs des algorithmes

### 2019 Metric Learning for Dynamic Text Classification [EN COURS]
**probl√®me** : en classif de textes, des labels peuvent parfois √™tre ajout√©s ou supprim√©s dans le temps => **classif dynamique**

**solution propos√©e** : remplacer la couche de sortie de classif des r√©seaux de neurones par un **espace m√©trique appris √† moindre co√ªt et s√©matiquement significatif** pour am√©liorer les performances du kNN

**context**:
* pb des classifs traditionnelles:
  * sortie de taille fixe == nb de labels i.e ajout / suppr de label ==> changer l'architecture du model
  * impossible de r√©utiliser les param√®tres d√©j√† appris ==> pb car les nvls classes n'ont souvent que tr√®s peu de donn√©es
  * pas d'exploitation de l'info entre label ==> affibli l'adaptation aux nouveaux labels

## misc-suggested

### 2020-LuyuGao-ModularizedTransfomerBasedRankingFramework

**voir aussi**:
* pr√©sentation vid√©o : https://slideslive.com/38939359
* code TBA : https://github.com/luyug/MORES


### 2020-MujeenSung-BiomedicalEntityReprWithSynonymMarginalization


## speech recognition

### 2020-MLSALargeScaleMultilingualDatasetForSpeechResearch-VineelPratap

**voir aussi** :
* wav2vec : https://github.com/pytorch/fairseq/tree/master/examples/wav2vec
* ASR avec wav2vec : https://github.com/facebookresearch/flashlight/tree/master/flashlight/app/asr
* [Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters](https://arxiv.org/pdf/2007.03001.pdf)

## TEMPLATE
### PDF FILE NAME
**Contexte** :

**Probl√®me** :

**Hypoth√®ses** :

**Existant**

**Solution**

**Question**

**voir aussi**
